<?xml version="1.1" encoding="UTF-8" standalone="no"?><project>
  <actions/>
  <description>smoke环境dataset更新</description>
  <keepDependencies>false</keepDependencies>
  <properties>
    <hudson.plugins.jira.JiraProjectProperty plugin="jira@3.3"/>
    <jenkins.model.BuildDiscarderProperty>
      <strategy class="hudson.tasks.LogRotator">
        <daysToKeep>30</daysToKeep>
        <numToKeep>1000</numToKeep>
        <artifactDaysToKeep>-1</artifactDaysToKeep>
        <artifactNumToKeep>-1</artifactNumToKeep>
      </strategy>
    </jenkins.model.BuildDiscarderProperty>
    <com.gitee.jenkins.connection.GiteeConnectionProperty plugin="gitee@1.2.4">
      <giteeConnection>gitee</giteeConnection>
    </com.gitee.jenkins.connection.GiteeConnectionProperty>
    <com.sonyericsson.rebuild.RebuildSettings plugin="rebuild@1.32">
      <autoRebuild>false</autoRebuild>
      <rebuildDisabled>false</rebuildDisabled>
    </com.sonyericsson.rebuild.RebuildSettings>
    <hudson.model.ParametersDefinitionProperty>
      <parameterDefinitions>
        <hudson.model.StringParameterDefinition>
          <name>DATASET_DIR</name>
          <description>需要更新的数据集目录（根目录为/data/nfs/dataset/workspace/mindspore_dataset），例如：mslite</description>
          <trim>false</trim>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>DATASET_URL</name>
          <description>华为云运行节点数据集更新链接：http://124.70.19.41/dataset/workspace/mindspore_dataset/mslite/</description>
          <trim>false</trim>
        </hudson.model.StringParameterDefinition>
        <org.jvnet.jenkins.plugins.nodelabelparameter.NodeParameterDefinition plugin="nodelabelparameter@1.8.1">
          <name>RUN_NODE</name>
          <description>任务运行节点：蓝区物理数据更新，选择smoke-nfs-30；华为云容器更新数据集，选择smoke-jumper-220-jenkins</description>
          <allowedSlaves>
            <string>smoke-jumper-220-jenkins</string>
            <string>smoke-nfs-30</string>
          </allowedSlaves>
          <defaultSlaves>
            <string>smoke-nfs-30</string>
          </defaultSlaves>
          <triggerIfResult>multiSelectionDisallowed</triggerIfResult>
          <allowMultiNodeSelection>false</allowMultiNodeSelection>
          <triggerConcurrentBuilds>false</triggerConcurrentBuilds>
          <ignoreOfflineNodes>false</ignoreOfflineNodes>
          <nodeEligibility class="org.jvnet.jenkins.plugins.nodelabelparameter.node.IgnoreOfflineNodeEligibility"/>
        </org.jvnet.jenkins.plugins.nodelabelparameter.NodeParameterDefinition>
        <hudson.model.ChoiceParameterDefinition>
          <name>DELETE_ORIGINAL_DIR</name>
          <description>是否删除原来目录</description>
          <choices class="java.util.Arrays$ArrayList">
            <a class="string-array">
              <string>no</string>
              <string>yes</string>
            </a>
          </choices>
        </hudson.model.ChoiceParameterDefinition>
        <com.cwctravel.hudson.plugins.extended__choice__parameter.ExtendedChoiceParameterDefinition plugin="extended-choice-parameter@0.82">
          <name>NODES_IP</name>
          <description>nodes ip</description>
          <quoteValue>false</quoteValue>
          <saveJSONParameterToFile>false</saveJSONParameterToFile>
          <visibleItemCount>30</visibleItemCount>
          <type>PT_CHECKBOX</type>
          <value>8.92.9.56,8.92.9.57,8.92.9.58,8.92.9.60,8.92.9.61,8.92.9.71,8.92.9.73,8.92.9.85,8.92.9.86,8.92.9.87,8.92.9.88,8.92.9.89,8.92.9.35,8.92.9.36,8.92.9.37,8.92.9.42,8.92.9.45,8.92.9.76,8.92.9.77,8.92.9.124,8.92.9.125,8.92.9.126,8.92.9.83,8.92.9.96,8.92.9.69,8.92.9.127,8.92.9.131,8.92.9.200,8.92.9.201,8.92.9.202</value>
          <multiSelectDelimiter>,</multiSelectDelimiter>
          <descriptionPropertyValue>8.92.9.56 (smoke-ascend-56),8.92.9.57 (smoke-ascend-57),8.92.9.58 (smoke-ascend-58),8.92.9.60 (smoke-ascend-60),8.92.9.61 (smoke-ascend-61),8.92.9.71 (smoke-ascend-71),8.92.9.73 (smoke-ascend-73),8.92.9.85 (smoke-ascend-85),8.92.9.86 (smoke-ascend-86),8.92.9.87 (smoke-ascend-87),8.92.9.88 (smoke-ascend-88),8.92.9.89 (smoke-ascend-89),8.92.9.35 (smoke-gpu-35),8.92.9.36 (smoke-gpu-36),8.92.9.37 (smoke-gpu-37),8.92.9.42 (smoke-gpu-42),8.92.9.45 (smoke-gpu-45),8.92.9.76 (smoke-gpu-76),8.92.9.77 (smoke-gpu-77),8.92.9.124 (smoke-gpu-124),8.92.9.125 (smoke-gpu-125),8.92.9.126 (smoke-gpu-126),8.92.9.83 (smoke-ascend310-83),8.92.9.96 (smoke-ascend310-96),8.92.9.69 (smoke-lite-69),8.92.9.127 (smoke-lite-127),,8.92.9.131 (smoke-lite-131),8.92.9.200 (smoke-lite-200),8.92.9.201 (smoke-lite-201),8.92.9.202 (smoke-lite-202)</descriptionPropertyValue>
          <projectName>Dataset_Update</projectName>
        </com.cwctravel.hudson.plugins.extended__choice__parameter.ExtendedChoiceParameterDefinition>
      </parameterDefinitions>
    </hudson.model.ParametersDefinitionProperty>
    
  </properties>
  <scm class="hudson.scm.NullSCM"/>
  <canRoam>true</canRoam>
  <disabled>false</disabled>
  <blockBuildWhenDownstreamBuilding>false</blockBuildWhenDownstreamBuilding>
  <blockBuildWhenUpstreamBuilding>false</blockBuildWhenUpstreamBuilding>
  <triggers/>
  <concurrentBuild>false</concurrentBuild>
  <builders>
    <hudson.tasks.Shell>
      <command>#!/bin/bash

# Init parameter
SSHPASS="sshpass -p ${BLUE_SMOKE_PASSWORD}"
SSH="${SSHPASS} ssh -q -o StrictHostKeychecking=no"
NODE_IP_LIST=(${NODES_IP//,/ })
TIMEOUT_CHECK_NETWORK=400
TIMEOUT_CHECK_SSH=200
DATASET_SOC_ROOT=/data/nfs/dataset/workspace/mindspore_dataset
if [ -n "${DATASET_URL}" ]; then
    DATASET_DES="/share_data/dataset/mindspore_dataset"
else
    if [[ "${DATASET_DIR}" =~ "lite_update" ]]; then
        NEW_DADASET_DIR="${DATASET_DIR//lite_update\//}"
    else
        NEW_DADASET_DIR="${DATASET_DIR}"
    fi
    DATASET_DES="/home/workspace/mindspore_dataset/${NEW_DADASET_DIR}"
fi


##################
# Common Library #
##################
# Color
Red='\e[0;31m'          # Red
Green='\e[0;32m'        # Green
BRed='\e[1;31m'         # Red
BGreen='\e[1;32m'       # Green
BCyan='\e[1;36m'        # Cyan
Purple='\e[0;35m'       # Purple
BPurple='\e[1;35m'      # Bold Purple
Color_Off='\e[0m'       # Text Reset

# Log head
function LOG_HEAD() {
    local assert_msg=${1}
    echo -e "\n${BGreen}[INFO] ${assert_msg}${Color_Off}"
}

# Log info
function LOG_INFO() {
    local assert_msg=${1}
    echo -e "${Green}[INFO] ${assert_msg}${Color_Off}"
}

# Log error
function LOG_ERROR() {
    local assert_msg=${1}
    echo -e "${BRed}[ERROR] ${assert_msg}${Color_Off}"
}

# Assert A is equal to B
# True: equal
# False: not equal
function DP_ASSERT_EQUAL() {
    local actual_value=${1}
    local expect_value=${2}
    local assert_msg=${3}
    local b_flag=${4:-"true"}
    if [ "${actual_value}" != "${expect_value}" ]; then
        LOG_ERROR "${assert_msg} is failed."
        echo "ret:${actual_value}"
        if [ "${b_flag}" = "true" ]; then
            exit 1
        fi
    else
        LOG_INFO "${assert_msg} is success."
    fi
}

# Excute command
function EXCUTE_COMMAND_REMOTE() {
    local node_ip=${1}
    local cmd=${2}
    echo -e "${BPurple}[Command]${Color_Off} ${Purple}${cmd}${Color_Off}"
    ${SSH} ${BLUE_SMOKE_USER}@${node_ip} "${cmd}"
    return $?
}


###########
# EXtract #
###########
# Check upgrade dataset mode
if [ -n "${DATASET_URL}" ]; then
    if [[ "${RUN_NODE}" != "smoke-jumper-220-jenkins" ]]; then
        LOG_ERROR "The running node must be smoke-jumper-220-jenkins, please check."
        exit 1
    fi
    if [[ "${DATASET_URL}" =~ "lite_update" ]]; then
        NEW_DADASET_DIR=$(echo ${DATASET_URL}|awk -F'/mindspore_dataset/lite_update/' '{print $NF}')
        CUT_DIR=4
    else
        NEW_DADASET_DIR=$(echo ${DATASET_URL}|awk -F'/mindspore_dataset/' '{print $NF}')
        CUT_DIR=3
    fi
    if [ "${DELETE_ORIGINAL_DIR}" = "yes" ] &amp;&amp; [[ ! "${DATASET_DIR}" =~ "lite_update" ]]; then
    	if [ -n "${NEW_DADASET_DIR}" ] &amp;&amp; [[ "${NEW_DADASET_DIR}" != "/" ]]; then
    		cd ${DATASET_DES}
    		rm -fr ${NEW_DADASET_DIR}
    		mkdir -p ${NEW_DADASET_DIR}
    	fi
    fi
    # Chmod u+x
    chmod -R u+w ${DATASET_DES}/${NEW_DADASET_DIR}

    # Download dataset
    cd ${DATASET_DES}/
    wget --http-user=${FTP_LOGING_USER} --http-password=${FTP_LOGING_PASSWORD} -r -q -nH -np --cut-dir=${CUT_DIR} -R index.html* ${DATASET_URL}
    DP_ASSERT_EQUAL "$?" "0" "Wget ${DATASET_URL} dataset to huaweicloud shara-data"
    ls -l ${DATASET_DES}/${NEW_DADASET_DIR}
else
    # Check upgrade dataset
    LOG_HEAD "Check upgrade dataset"
    if [ ! -d "${DATASET_SOC_ROOT}/${DATASET_DIR}" ]; then
        LOG_ERROR "The directory ${DATASET_DIR} not exist, please check."
        exit 1
    fi

    # Copy dataset
    LOG_HEAD "Copy dataset"
    process_pid=()
    for((i=0; i&lt;${#NODE_IP_LIST[@]}; i++)); do
        EXCUTE_COMMAND_REMOTE "${NODE_IP_LIST[i]}" "df -Th|grep '8.92.9.30:/data/nfs'"
        if [ $? -ne 0 ]; then
            EXCUTE_COMMAND_REMOTE "${NODE_IP_LIST[i]}" "mount -t nfs -o vers=3,nolock 8.92.9.30:/data/nfs /nfs"
            DP_ASSERT_EQUAL "$?" "0" "NODE[${NODE_IP_LIST[i]}] Mount nfs"
        fi
        if [ "${DELETE_ORIGINAL_DIR}" = "yes" ] &amp;&amp; [[ ! "${DATASET_DIR}" =~ "lite_update" ]]; then
            EXCUTE_COMMAND_REMOTE "${NODE_IP_LIST[i]}" "rm -fr ${DATASET_DES}; mkdir -p ${DATASET_DES}; cp -rf /nfs/dataset/workspace/mindspore_dataset/${DATASET_DIR}/* ${DATASET_DES}" &gt; ${WORKSPACE}/update_${NODE_IP_LIST[i]}.log 2&gt;&amp;1 &amp;
        else
            EXCUTE_COMMAND_REMOTE "${NODE_IP_LIST[i]}" "mkdir -p ${DATASET_DES}; cp -rf /nfs/dataset/workspace/mindspore_dataset/${DATASET_DIR}/* ${DATASET_DES}" &gt; ${WORKSPACE}/update_${NODE_IP_LIST[i]}.log 2&gt;&amp;1 &amp;
        fi
        process_pid[${i}]=$(echo $!)
    done

    # Wait for all process execution
    flag_check=0
    for((i=0; i&lt;${#NODE_IP_LIST[@]}; i++)); do
        wait ${process_pid[i]}
        ret=$?
        if [ "${ret}" -eq "0" ] || [ ! -f ${WORKSPACE}/update_${NODE_IP_LIST[i]}.log ]; then
            EXCUTE_COMMAND_REMOTE "${NODE_IP_LIST[i]}" "ls -l ${DATASET_DES}"
            LOG_INFO "Node[${NODE_IP_LIST[i]}] - Update dataset success."
        else
            cat update_${NODE_IP_LIST[i]}.log
            LOG_ERROR "Node[${NODE_IP_LIST[i]}] - Update dataset failed."
            cat ${WORKSPACE}/update_${NODE_IP_LIST[i]}.log
            flag_check=1
        fi
    done

    # Check copy ret
    if [ "${flag_check}" -ne "0" ]; then
        LOG_ERROR "Update dataset of all nodes is failed."
        exit 1
    else
        LOG_INFO "Update dataset of all nodes is success."
        exit 0
    fi
fi
</command>
      <configuredLocalRules/>
    </hudson.tasks.Shell>
  </builders>
  <publishers/>
  <buildWrappers>
    <hudson.plugins.ws__cleanup.PreBuildCleanup plugin="ws-cleanup@0.39">
      <deleteDirs>false</deleteDirs>
      <cleanupParameter/>
      <externalDelete/>
      <disableDeferredWipeout>false</disableDeferredWipeout>
    </hudson.plugins.ws__cleanup.PreBuildCleanup>
    <org.jenkinsci.plugins.credentialsbinding.impl.SecretBuildWrapper plugin="credentials-binding@1.25">
      <bindings>
        <org.jenkinsci.plugins.credentialsbinding.impl.UsernamePasswordMultiBinding>
          <credentialsId>user_root</credentialsId>
          <usernameVariable>BLUE_SMOKE_USER</usernameVariable>
          <passwordVariable>BLUE_SMOKE_PASSWORD</passwordVariable>
        </org.jenkinsci.plugins.credentialsbinding.impl.UsernamePasswordMultiBinding>
        <org.jenkinsci.plugins.credentialsbinding.impl.UsernamePasswordMultiBinding>
          <credentialsId>LOGS_FTP</credentialsId>
          <usernameVariable>FTP_LOGING_USER</usernameVariable>
          <passwordVariable>FTP_LOGING_PASSWORD</passwordVariable>
        </org.jenkinsci.plugins.credentialsbinding.impl.UsernamePasswordMultiBinding>
      </bindings>
    </org.jenkinsci.plugins.credentialsbinding.impl.SecretBuildWrapper>
    <hudson.plugins.timestamper.TimestamperBuildWrapper plugin="timestamper@1.13"/>
    <hudson.plugins.ansicolor.AnsiColorBuildWrapper plugin="ansicolor@1.0.0">
      <colorMapName>xterm</colorMapName>
    </hudson.plugins.ansicolor.AnsiColorBuildWrapper>
  </buildWrappers>
</project>